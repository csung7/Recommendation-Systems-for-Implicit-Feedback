{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "written by **`Chul Sung`**, Data Scientist @ IBM Cloud\n",
    "**LinkedIn:** https://www.linkedin.com/in/chul-sung-8b073923 **Twitter:** `@csung7`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color: #0000A0; font-size: 18pt; font-weight: bold;\">\n",
    "Welcome to IBM `Apache Spark` Service!\n",
    "</span>\n",
    "\n",
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "**Recommendation Systems for Implicit Feedback**\n",
    "</span>\n",
    "\n",
    "---\n",
    "Through this tutorial you will answer the following questions:\n",
    "* What Are Apache Spark DataFrames? \n",
    "* How are Window Functions different from Aggregates or user-defined functions (UDFs)?\n",
    "* How to Analyze Data with Spark DataFrames?\n",
    "* How to Build a Machine Learning Model with Spark ML?\n",
    "* How to Evaluate the Model?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #0000A0; font-size: 18pt; font-weight: bold;\">\n",
    "What Are `Spark DataFrames`?\n",
    "</span>\n",
    "    \n",
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "**Spark DataFrames** are conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. Once created, it can be manipulated using a variety of functions for getting basic information about the DataFrame.\n",
    "</span>\n",
    "\n",
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "Spark DataFrames have two types of functions: **transformations** and **actions**. If a function of dataframe returns a dataframe then it is a transformation function like map() or filter(), whereas if its function returns **something else** other than a dataframe, then it is an **action** function like count() or collect(). I mentioned two types of functions because all transformations in Spark are completely **lazy**, and Spark does not begin computing the partitions until an action is called.\n",
    "</span>\n",
    "\n",
    "<span style=\"font-size: 12pt; font-weight: bold;\">\n",
    "[References]\n",
    "</span>\n",
    "* [High Performance Spark - Publisher: O'Reilly Media](http://shop.oreilly.com/product/0636920046967.do)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 18pt; font-weight: bold;\">\n",
    "Then Why **DataFrames** instead of RDD?\n",
    "</span>\n",
    "<img src=\"https://databricks.com/wp-content/uploads/2015/02/Screen-Shot-2015-02-16-at-9.46.39-AM.png\">\n",
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "The above chart compares the runtime performance of running group-by-aggregation on 10 million integer pairs on a single machine. Through the new DataFrame API, **Python programs** can achieve the **same level of performance** as JVM programs because the Catalyst optimizer compiles DataFrame operations into JVM bytecode. Indeed, performance sometimes beats hand-written Scala code.\n",
    "</span>\n",
    "\n",
    "\n",
    "<span style=\"font-size: 12pt; font-weight: bold;\">\n",
    "[References]\n",
    "</span>\n",
    "* [Recent performance improvements in Apache Spark: SQL, Python, DataFrames, and More](https://databricks.com/blog/2015/04/24/recent-performance-improvements-in-apache-spark-sql-python-dataframes-and-more.html)\n",
    "* [Pyspark SQL DataFrame Docs](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #0000A0; font-size: 18pt; font-weight: bold;\">\n",
    "Spark applications\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "Each Spark application runs independently on a cluster, coordinated by the SparkContext object in your main program (called the **driver** program). Once the SparkContext connects to worker nodes through the cluster manager, Spark acquires **executors** on nodes in the cluster, which are processes that run computations and store data for your application. \n",
    "</span>\n",
    "\n",
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "When you create a notebook, SparkContext is already created for you, with the variable name **`sc`**.\n",
    "</span>\n",
    "\n",
    "<img src=\"http://spark.apache.org/docs/latest/img/cluster-overview.png\">\n",
    "\n",
    "<span style=\"font-size: 12pt; font-weight: bold;\">\n",
    "[References]\n",
    "</span>\n",
    "* [SparkContext - Entry Point to Spark](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sparkcontext.html)\n",
    "* [Cluster Mode Overview](http://spark.apache.org/docs/latest/cluster-overview.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "With SparkContext **`sc`** you can check your spark version and it determines how many resources are allotted to each executor:\n",
    "</span>\n",
    "\n",
    "<span style=\"font-size: 12pt; font-weight: bold;\">\n",
    "[References]\n",
    "</span>\n",
    "* [Available Properties](http://spark.apache.org/docs/latest/configuration.html#available-properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sc._conf.get('spark.driver.cores')\n",
    "print sc._conf.get('spark.driver.memory')\n",
    "\n",
    "print sc._conf.get('spark.executor.cores')\n",
    "print sc._conf.get('spark.executor.memory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #0000A0; font-size: 18pt; font-weight: bold;\">\n",
    "Let's Get Started `Recommendation Systems for Implicit Feedback with PySpark`.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #0000A0; font-size: 18pt; font-weight: bold;\">\n",
    "Create `Spark DataFrames` from your database.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "As the SparkContext is the entry point for all Spark applications, **`HiveContext`** and **`SQLContext`** objects serve as the entry points for Spark SQL, helping to connect your database through JDBC connector and create a Spark dataframe. **`HiveContext`** is a super set of the SQLContext in Spark < 2.0. One of the key differences is using `HiveContext` you can use the **window function** feature. Spark 2.0 provides `SparkSession` is the new entry point of Spark which is combination of SQLContext, HiveContext and future StreamingContext.\n",
    "</span>\n",
    "\n",
    "<span style=\"font-size: 12pt; font-weight: bold;\">\n",
    "[References]\n",
    "</span>\n",
    "* [Understanding Spark’s SparkConf, SparkContext, SQLContext and HiveContext](https://blogs.msdn.microsoft.com/bigdatasupport/2015/09/14/understanding-sparks-sparkconf-sparkcontext-sqlcontext-and-hivecontext/)\n",
    "* [Difference between Spark HiveContext and SQLContext](http://www.openkb.info/2016/02/difference-between-spark-hivecontext.html)\n",
    "* [What is the difference between Apache Spark SQLContext vs HiveContext?](http://stackoverflow.com/questions/33666545/what-is-the-difference-between-apache-spark-sqlcontext-vs-hivecontext)\n",
    "* [Introduction to Spark 2.0 - Part 1 : Spark Session API](http://blog.madhukaraphatak.com/introduction-to-spark-two-part-1/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #0000A0; font-size: 18pt; font-weight: bold;\">\n",
    "What are `Window Functions`?\n",
    "</span>\n",
    "\n",
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "**Window functions** are complementary to existing DataFrame operations: **aggregates** such as sum and avg and ** user-defined functions (UDFs)**.  **Aggregates** return **one result**, a sum or average, for each group of records, while **UDFs** return one result for **each record** based on only data in that record. In contrast, **window functions** return one result for **each record** based on **a window of records**. With **Window Spec definition** you can specify a window spec which has three components: **partition by**, **order by**, and **frame** (or boundaries).  In this lecture, I will show you an example of **window functions**.\n",
    "</span>\n",
    "\n",
    "<span style=\"font-size: 12pt; font-weight: bold;\">\n",
    "[References]\n",
    "</span>\n",
    "* [Spark Window Functions for DataFrames and SQL](http://xinhstechblog.blogspot.com/2016/04/spark-window-functions-for-dataframes.html)\n",
    "* [UDFs — User-Defined Functions](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-udfs.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "Import `HiveContext` module.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, HiveContext\n",
    "\n",
    "#sqlContext = SQLContext(sc)\n",
    "sqlContext = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "To connect to our database and create a dataframe for our data with the Spark SQL instance **`sqlContext`**, we need two key parameters: (1) our database JDBC URL (including Host name, Port number, Database name, User ID, and Password) and (2) a specific table name.\n",
    "</span>\n",
    "\n",
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "**dashDB:** When you load your data into dashDB, in the **Connection Information** page it will show you **JDBC URL string**.\n",
    "This is my dashDB JDBC URL: **`\n",
    "jdbc:db2://awh-yp-small03.services.dal.bluemix.net:50001/BLUDB:user=dash107595;password=U4A5pYgThdbo;sslConnection=true;\n",
    "`**\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dashdb_url = 'jdbc:db2://awh-yp-small03.services.dal.bluemix.net:50001/BLUDB:user=dash107595;password=U4A5pYgThdbo;sslConnection=true;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dashdb_table = 'DASH107595.T_ONLINE_RETAIL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_retail_data = sqlContext.read.format('jdbc').options(url=dashdb_url, dbtable=dashdb_table).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "The Spark dataframe **`printSchema()`** method show us the schema of a DataFrame.\n",
    "</span>\n",
    "\n",
    "<span style=\"font-size: 12pt; font-weight: bold;\">\n",
    "[References]\n",
    "</span>\n",
    "* [JSON Datasets](http://spark.apache.org/docs/latest/sql-programming-guide.html#json-datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_retail_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "Now that the online retail data has been loaded into a Spark dataframe, we can see what is in it using the dataframe **`show()`** function with **`numRows`** and **`truncate`** options. The **show()** function provides different parameters such as\n",
    "```\n",
    "df_retail_data.show(numRows=3, truncate=False)\n",
    "df_retail_data.show(truncate=False)\n",
    "df_retail_data.show(numRows=3)\n",
    "```\n",
    "</span>\n",
    "\n",
    "<span style=\"font-size: 12pt; font-weight: bold;\">\n",
    "[References]\n",
    "</span>\n",
    "* [Creating DataFrames](http://spark.apache.org/docs/latest/sql-programming-guide.html#creating-dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_retail_data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "The datafame includes the invoice number (`INVOICENO`) for different purchases, along with the stock code (`STOCKCODE` as an item ID), an item description (`DESCRIPTION`), the number purchased (`QUANTITY`), the date of purchase (`INVOICEDATE`), the price of the items (`UNITPRICE`), a customer ID (`CUSTOMERID`), and the country of origin for the customer (`COUNTRY`).\n",
    "</span>\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "When trying to collect some summary statistics for the DataFrame, there is a built-in DataFrame **`describe()`** function, returning number of non-null entries (count), mean, standard deviation, and minimum and maximum value for each numerical column.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_retail_data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "It can also be limited to certain columns:\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_retail_data.describe('STOCKCODE','CUSTOMERID').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "Most columns have no missing values, but Customer ID (**`CUSTOMERID`**) is missing in several rows. If the customer ID is missing, we don't know who bought the item. We should drop these rows from our data first. We can use the DataFrame **`dropna()`** function to drop the rows with missing data, sepeficially in the Customer ID column.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_cleaned_retail = df_retail_data.dropna(subset='CUSTOMERID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "When we have more than one column in the subset, with the **`how`** parameter you can drop the rows that **any** NA values are present or **all** values are NA.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_retail_data.dropna(subset=['STOCKCODE','CUSTOMERID'], how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_cleaned_retail.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #0000A0; font-size: 18pt; font-weight: bold;\">Mission 1. Create  `Item IDs` with Integer Type.</span>\n",
    "\n",
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "As you can see, **STOCKCODE** values consist of numbers and letters so the column type is **string** but we need the item IDs with a numerical type. To successfully complete this mission:</span>\n",
    "\n",
    "---\n",
    "1. Create a unique STOCKCODE dataframe from the **`df_cleaned_retail`** dataframe with **`dropDuplicates()`** function.\n",
    "2. Create sequential numbers for the unique STOCKCODE column with the **`row_number()`** window function, saving the numbers into new `ITEM_ID` column. And then save the output dataframe into **`df_unique_item_ids_int`**. \n",
    "3. Do left join your new **`df_unique_item_ids_int`** dataframe into **`df_cleaned_retail`** dataframe on **`STOCKCODE`** column and create a new dataframe naming **`df_retail_data_w_item_ids`** including **`STOCKCODE, ITEM_ID, QUANTITY, CUSTOMERID`** columns.\n",
    "---\n",
    "\n",
    "<span style=\"font-size: 12pt; font-weight: bold;\">\n",
    "[References]\n",
    "</span>\n",
    "* [DataFrame.dropDuplicates()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)\n",
    "* [pyspark.sql.functions.row_number()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)\n",
    "* [DataFrame.join(other, on=None, how=None)](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sqlfunc\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Create a unique item list.\n",
    "#df_unique_item_ids = \n",
    "\n",
    "# 2. And then with the `row_number` window function, assign the sequential numbers to the unique item IDs,\n",
    "# creating a new column named `ITEM_ID`.\n",
    "#wSpec = \n",
    "#df_unique_item_ids_int = \n",
    "\n",
    "# 3.  Combine the unique item list.\n",
    "#df_retail_data_w_item_ids = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "For future reference, we are going to create item lookup dataframe including the an item description (**`DESCRIPTION`**).\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_item_lookup = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #0000A0; font-size: 18pt; font-weight: bold;\">Mission 2. Group **`Purchase Quantity`** by Stock Code and Customer ID.</span>\n",
    "\n",
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "We need to combine the purchase quantity of the same stock codes from same customer IDs. In order to do that, we are going to group together by stock code and customer ID **`['STOCKCODE','ITEM_ID','CUSTOMERID']`**. For example, customer A ordered product P 3 first and ordered product P 4 again. Then we will have two rows for the customer A invoices. We need to combine the quantities to 7.\n",
    "</span>\n",
    "\n",
    "---\n",
    "1. Group purchase quantity together by stock code and customer ID **`['STOCKCODE','ITEM_ID','CUSTOMERID']`** with **`groupBy()`** function and save the output dataframe into **`df_retail_data_w_item_ids_sum`**.\n",
    "2. Once you grouped the quantities, change any sums that equal zero to one with **`when() and otherwise()`** function and save the output dataframe into **`df_retail_data_w_item_ids_sum`**.\n",
    "3. Only include customers with a positive purchase total to eliminate possible errors with **`filter()`** function and save the output dataframe into **`df_purchased_retail_data`**.\n",
    "---\n",
    "\n",
    "<span style=\"font-size: 12pt; font-weight: bold;\">\n",
    "[References]\n",
    "</span>\n",
    "* [DataFrame.groupBy()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData)\n",
    "* [pyspark.sql.functions.when()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.when)\n",
    "* [DataFrame.filter()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Group purchase quantity together by stock code ('STOCKCODE','ITEM_ID') and customer ID ('CUSTOMERID').\n",
    "\n",
    "#df_retail_data_w_item_ids_sum = \n",
    "\n",
    "# 2. Change any sums that equal zero to one.\n",
    "#df_retail_data_w_item_ids_sum = \n",
    "\n",
    "# 3. Only include customers with a positive purchase total to eliminate possible errors.\n",
    "#df_purchased_retail_data = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #0000A0; font-size: 18pt; font-weight: bold;\">Create a **`Training and Test Set`**.</span>\n",
    "\n",
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "In Machine Learning applications, we need to test whether the model we trained with the training set is any good on the new test set. We can randomly split the feature matrix into two groups and use one for training and the other for test set. However, it would be possible for some products purchased by the same users to assign into training and test sets separately. To avoid this case, we are going to randomly split customer data into two groups and based on the two groups of the customers we can create training and test sets.\n",
    "</span>\n",
    "\n",
    "---\n",
    "1. Randomly split the customer list into **two groups**.\n",
    "2. Using the first customer group, pull all products they ordered and create a complete **training set**.\n",
    "3. Using the second customer group, pull all products they ordered and create a complete **test set**.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Randomly split the customers into **two groups** for training and test set.\n",
    "df_user_lookup = df_purchased_retail_data.select(['CUSTOMERID']).dropDuplicates()\n",
    "df_user_splits = df_user_lookup.randomSplit([8.0, 2.0])\n",
    "\n",
    "# 2. Create a complete **training set** with the first customer list.\n",
    "df_training_data = df_purchased_retail_data.join(df_user_splits[0], df_purchased_retail_data.CUSTOMERID == df_user_splits[0].CUSTOMERID,\n",
    "                                             how='inner').select(df_purchased_retail_data.CUSTOMERID,'ITEM_ID','STOCKCODE','QUANTITY')\n",
    "\n",
    "# 3. Create a complete **test set** with the second customer list.\n",
    "df_test_data = df_purchased_retail_data.join(df_user_splits[1], df_purchased_retail_data.CUSTOMERID == df_user_splits[1].CUSTOMERID,\n",
    "                                             how='inner').select(df_purchased_retail_data.CUSTOMERID,'ITEM_ID','STOCKCODE','QUANTITY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #0000A0; font-size: 18pt; font-weight: bold;\">Implement **`Model-based Collaborative Filtering for Implicit Feedback`**.</span>\n",
    "\n",
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "Collaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user-item association matrix. **Spark ML** (**`pyspark.ml`**) is a package of machine learning and statistics algorithms written with Spark, built on top of Spark SQL dataframes. And **Spark ML** currently supports **model-based collaborative filtering**, in which users and products are described by a small set of latent factors that can be used to predict missing entries. **`Spark ML`** uses the **alternating least squares (ALS) algorithm** to learn these latent factors. In the **`Spark ML`** the **ALS algorithm** is able to handle **explicit feedback** or **implicit feedback** data.\n",
    "</span>\n",
    "\n",
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "In this tutorial we are using **implicit feedback** data so I will show you how to model implicit-feedback based recommender systems. This approach was developed on the following paper [Hu, Koren, and Volinsky](http://yifanhu.net/PUB/cf.pdf).\n",
    "Basically we need to turn the ratings matrix into a confidence matrix with the following equation:\n",
    "$$C_{ui}=1+\\alpha r_{ui}$$ where $C_{ui}$ is the confidence matrix for our users $u$ and our items $i$. The $\\alpha$ term represents a linear scaling of the rating preferences (in our case number of purchases) and the $r$ term is our original matrix of purchases. The paper suggests 40 as a good starting point for $\\alpha$.\n",
    "</span>\n",
    "\n",
    "---\n",
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "For more details on the Collaborative Filtering, please take a look at the below contents later. The implementation in **spark.ml** has the following parameters:\n",
    "* **rank** is the number of latent factors in the model (defaults to 10). The number of latent features in the user/item feature vectors. The paper recommends varying this between 20-200. Increasing the number of features may overfit but could reduce bias.\n",
    "* **maxIter** is the maximum number of iterations to run (defaults to 10). The number of times to alternate between both user feature vector and item feature vector in alternating least squares. More iterations will allow better convergence at the cost of increased computation. The authors found 10 iterations was sufficient, but more may be required to converge.\n",
    "* **regParam** specifies the regularization parameter in ALS (defaults to 1.0). Used for regularization during alternating least squares. Increasing this value may increase bias but decrease variance.\n",
    "    * **Bias** is high if the concept class cannot model the true data distribution well, and does not depend on training set size (underfitting: when you have high bias).\n",
    "    * **Variance** depends on the training set size. It decreases with more training data, and increases with more complicated classifiers (overfitting: when you have extra high variance).\n",
    "* **implicitPrefs** specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data (defaults to false which means using explicit feedback). Implicit weighted ALS from Hu, Koren, and Volinsky 2008. Designed for alternating least squares and implicit feedback based collaborative filtering.\n",
    "* **alpha** is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations (defaults to 1.0). The parameter associated with the confidence matrix discussed in the paper, where Cui = 1 + alpha*Rui. The paper found a default of 40 most effective. Decreasing this will decrease the variability in confidence between various ratings.\n",
    "    \n",
    "\n",
    "**Explicit vs. implicit feedback**\n",
    "The standard approach to matrix factorization based collaborative filtering treats the entries in the user-item matrix as explicit preferences given by the user to the item, for example, users giving ratings to movies.\n",
    "\n",
    "It is common in many real-world use cases to only have access to **implicit feedback** (e.g. views, clicks, purchases, likes, shares etc.). The approach used in **spark.ml** to deal with such data is taken from [Hu, Koren, and Volinsky](http://yifanhu.net/PUB/cf.pdf). Essentially, instead of trying to model the matrix of ratings directly, this approach treats the data as numbers representing the strength in observations of user actions (such as the number of clicks, or the cumulative duration someone spent viewing a movie). Those numbers are then related to the level of confidence in observed user preferences, rather than explicit ratings given to items. The model then tries to find latent factors that can be used to predict the expected preference of a user for an item.\n",
    "\n",
    "**[ALS.scala code](https://github.com/apache/spark/blob/d6dc12ef0146ae409834c78737c116050961f350/mllib/src/main/scala/org/apache/spark/ml/recommendation/ALS.scala)**\n",
    "```\n",
    "if (implicitPrefs) {\n",
    "  // Extension to the original paper to handle b < 0. confidence is a function of |b|\n",
    "  // instead so that it is never negative. c1 is confidence - 1.0.\n",
    "  val c1 = alpha * math.abs(rating)\n",
    "  // For rating <= 0, the corresponding preference is 0. So the term below is only added\n",
    "  // for rating > 0. Because YtY is already added, we need to adjust the scaling here.\n",
    "  if (rating > 0) {\n",
    "    numExplicits += 1\n",
    "    ls.add(srcFactor, (c1 + 1.0) / c1, c1)\n",
    "}\n",
    "```\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "'''\n",
    "If the rating matrix is derived from another source of information (i.e. it is inferred from other signals),\n",
    "you can set implicitPrefs to True to get better results:\n",
    "'''\n",
    "\n",
    "rank = 25\n",
    "implicitPrefs=True\n",
    "iterations = 15\n",
    "regularization_parameter = 0.1\n",
    "alpha = 40.0\n",
    "#seed = 5L\n",
    "\n",
    "als = ALS(rank=rank, maxIter=iterations, regParam=regularization_parameter, alpha=alpha, implicitPrefs=implicitPrefs,\n",
    "          userCol=\"CUSTOMERID\", itemCol=\"ITEM_ID\", ratingCol=\"QUANTITY\")\n",
    "\n",
    "# Training the ALS model.\n",
    "als_model = als.fit(df_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #0000A0; font-size: 18pt; font-weight: bold;\">Predict **`Confidence`** for Test Set.</span>\n",
    "\n",
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "It will create a new column named **`prediction`** with recommendation confidence scores.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_predictions = als_model.transform(df_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #0000A0; font-size: 18pt; font-weight: bold;\">Mission 3. Evaluate the **`ALS Recommender System`**.</span>\n",
    "\n",
    "\n",
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "The following paper [Hu, Koren, and Volinsky](http://yifanhu.net/PUB/cf.pdf) introduced how to evealute an ALS recommender system for implicit feedback: We denote by $rank_{ui}$ the **percentile-ranking of product** $i$ within the ordered list of all products prepared **for user $u$**. This way, $rank_{ui}$ = 0% would mean that product $i$ is predicted to be the most desirable for user $u$, thus preceding all other products in the list. On the other hand, $rank_{ui}$ = 100% indicates that product $i$ is predicted to be the least\n",
    "preferred for user $u$, thus placed at the end of the list. (We opted for using percentile-ranks rather than absolute ranks in order to make our discussion general and independent of the number of programs.) Our basic quality measure is the expected percentile ranking of a watching unit in the test period, which is: $$\\overline{rank}=\\frac{\\sum_{u,i}r_{ui}rank_{ui}}{\\sum_{u,i}r_{ui}}$$ Lower values of $\\overline{rank}$ are more desirable, as they indicate ranking actually watched shows closer to the top of the recommendation lists. Notice that for random predictions, the expected value of $rank_{ui}$ is 50% (placing $i$ in the middle of the sorted list). Thus, $\\overline{rank}\\geqslant 50\\%$ indicates an algorithm no better than random.\n",
    "</span>\n",
    "\n",
    "---\n",
    "1. Filter out the prediction results which our model could not calculate their confidence scores and returned NaN. We are using **`filter()`** and **`isnan()`** function and save the output dataframe into **`df_predictions_filtered`**.\n",
    "2. Convert the confidence scores of each customer to percentile-rank with **`percent_rank()`** window function. With **`windowSpecRank`** window spec definition, we need to partition our data by each customer. And then calcuate the percentile-rank, saving the output column into the new **`PRANK`** column and save the output dataframe to **`df_predictions_pranks`**.\n",
    "3. Calcuate the new **`PRANK`** column times the **`QUANTITY`** column, saving the output column into the new **`RRANK`** column and save the output dataframe to **`df_pred_cooked`**. And then calculate sum of **`RRANK`** column (numerator) over sum of **`QUANTITY`** column (denominator) with **`sum()`** function.\n",
    "---\n",
    "\n",
    "<span style=\"font-size: 12pt; font-weight: bold;\">\n",
    "[References]\n",
    "</span>\n",
    "* [pyspark.sql.functions.isnan()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.isnan)\n",
    "* [pyspark.sql.functions.percent_rank()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.percent_rank)\n",
    "* [pyspark.sql.functions.sum()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.sum)\n",
    "* [DataFrame.filter()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Filter out the products which the model is not able to calculate the confidence score.\n",
    "#df_predictions_filtered = \n",
    "\n",
    "# 2. Convert the confidence scores to percentile-rank.\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#windowSpecRank = \n",
    "\n",
    "# pranks is r X rank\n",
    "#df_predictions_pranks = \n",
    "\n",
    "# 3. Calculate the numerator and denominator.\n",
    "#df_pred_cooked ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color: #0000A0; font-size: 18pt; font-weight: bold;\">A Recommendation Example.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "Check item lookup dataframe:\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The contents will not truncate.\n",
    "df_item_lookup.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "Select one of customers in the training set:\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_training_data.select('CUSTOMERID').dropDuplicates().show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_customer = 12431"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "Create a list for all of the products which my customer purchased:\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_product_list = df_cleaned_retail.filter(df_cleaned_retail['CUSTOMERID'] == my_customer).select('STOCKCODE').dropDuplicates().flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print my_product_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "Using the **`dataframe isin()`** function, pull the descriptions of the products in the list.\n",
    "</span>\n",
    "\n",
    "<span style=\"font-size: 12pt; font-weight: bold;\">\n",
    "[References]\n",
    "</span>\n",
    "* [dataframe isin()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_my_products = df_item_lookup.filter(df_item_lookup.STOCKCODE.isin(my_product_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_my_products.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_my_products.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "Using the **`dataframe isin()`** function, pull the products which the customer did not purchase.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_products_wo_mine = df_item_lookup.filter(~df_item_lookup.STOCKCODE.isin(my_product_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_products_wo_mine.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "Add my customer ID into the product dataframe which the customer did not purchase. \n",
    "New columns can be created only by using literals with **`pyspark.sql.functions.lit()`** function.\n",
    "</span>\n",
    "\n",
    "[References]\n",
    "</span>\n",
    "* [pyspark.sql.functions.lit()](https://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html#pyspark.sql.functions.lit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_products_wo_mine_w_my_cus_id = df_products_wo_mine.withColumn(\"CUSTOMERID\", sqlfunc.lit(my_customer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "Predict new products for my customer:\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_my_new_products_predict = als_model.transform(df_products_wo_mine_w_my_cus_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: monospace; font-size: 12pt;\">\n",
    "Top 10 highly recommended products:\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Filter out the products which the model is not able to calculate the confidence score.\n",
    "df_my_new_products_predict_filtered = df_my_new_products_predict.filter(~sqlfunc.isnan(\"prediction\"))\n",
    "df_my_new_products_predict_filtered.sort(df_my_new_products_predict_filtered.prediction.desc()).select(['STOCKCODE','DESCRIPTION']).show(10, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}